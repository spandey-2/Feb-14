{"cells":[{"cell_type":"code","execution_count":1,"id":"0c2e913e-97d5-4bcc-b390-026155929024","metadata":{"id":"0c2e913e-97d5-4bcc-b390-026155929024","executionInfo":{"status":"ok","timestamp":1737219052277,"user_tz":-330,"elapsed":7982,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"outputs":[],"source":["import numpy as np\n","import gensim.downloader as api\n","from scipy.special import softmax"]},{"cell_type":"code","execution_count":2,"id":"530082eb-134d-4003-b00e-4cb205098f84","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"530082eb-134d-4003-b00e-4cb205098f84","executionInfo":{"status":"ok","timestamp":1737219298607,"user_tz":-330,"elapsed":246333,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}},"outputId":"ed920f15-fece-4abe-a486-e35a5c343695"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading GloVe embeddings...\n","[==================================================] 100.0% 376.1/376.1MB downloaded\n","GloVe embeddings loaded.\n"]}],"source":["# Load pre-trained GloVe embeddings\n","print(\"Loading GloVe embeddings...\")\n","glove_vectors = api.load(\"glove-wiki-gigaword-300\")  # 300-dimensional GloVe embeddings\n","print(\"GloVe embeddings loaded.\")"]},{"cell_type":"code","execution_count":3,"id":"ac7dd97d-cb38-4391-b1a0-437549383daf","metadata":{"id":"ac7dd97d-cb38-4391-b1a0-437549383daf","executionInfo":{"status":"ok","timestamp":1737219298608,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"outputs":[],"source":["# Our input sentence\n","sentence = \"The dog chased the cat which was scared\"\n","words = sentence.split()\n","seq_length = len(words)"]},{"cell_type":"code","source":["seq_length"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIE7eqiUlum5","executionInfo":{"status":"ok","timestamp":1737219298608,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}},"outputId":"6d4af981-1d63-4949-9dae-e11ff466b5ba"},"id":"wIE7eqiUlum5","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":5,"id":"eab9a466-0bc4-4623-8830-95301f607423","metadata":{"id":"eab9a466-0bc4-4623-8830-95301f607423","executionInfo":{"status":"ok","timestamp":1737219298608,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"outputs":[],"source":["# Hyperparameters\n","embedding_dim = 300  # GloVe vectors are 300-dimensional\n","max_seq_length = 512  # Maximum sequence length for positional embeddings"]},{"cell_type":"code","execution_count":6,"id":"de14a167-4e7d-49c9-b3be-4533fe5a4e95","metadata":{"id":"de14a167-4e7d-49c9-b3be-4533fe5a4e95","executionInfo":{"status":"ok","timestamp":1737219303161,"user_tz":-330,"elapsed":352,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"outputs":[],"source":["def get_positional_encoding(seq_length, d_model, max_len=max_seq_length):\n","    positional_encoding = np.zeros((max_len, d_model))\n","\n","    for pos in range(max_len):\n","        for i in range(0, d_model, 2):\n","            positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))\n","            positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","    return positional_encoding[:seq_length]\n","\n","# Step 1: Create word embeddings using GloVe\n","word_embeddings = np.array([glove_vectors[word.lower()] if word.lower() in glove_vectors else glove_vectors['unk'] for word in words])\n","\n","# Step 2: Create positional embeddings\n","positional_embeddings = get_positional_encoding(seq_length, embedding_dim)\n","\n","# Step 3: Combine word embeddings and positional embeddings\n","input_embeddings = word_embeddings + positional_embeddings"]},{"cell_type":"code","source":["word_embeddings.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QQ_8cy8XT2pJ","executionInfo":{"status":"ok","timestamp":1737219307734,"user_tz":-330,"elapsed":414,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}},"outputId":"c7577093-cae8-414c-8833-35f10e79a6d2"},"id":"QQ_8cy8XT2pJ","execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 300)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["positional_embeddings.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ogIbd3heUxll","executionInfo":{"status":"ok","timestamp":1737219313432,"user_tz":-330,"elapsed":353,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}},"outputId":"78d57456-86c3-4345-89a7-aa9f60034cc4"},"id":"ogIbd3heUxll","execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 300)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["input_embeddings"],"metadata":{"id":"wnwRQ_SdU8k-","executionInfo":{"status":"ok","timestamp":1736666494835,"user_tz":-330,"elapsed":395,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}},"outputId":"400847a4-1128-46fe-e6c6-24ee55c91538","colab":{"base_uri":"https://localhost:8080/"}},"id":"wnwRQ_SdU8k-","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.04656   ,  1.21318001, -0.0074364 , ...,  1.0090611 ,\n","        -0.20988999,  1.053913  ],\n","       [ 0.73104098,  1.40159875,  0.84722602, ...,  1.33936   ,\n","         0.57994003,  1.068149  ],\n","       [ 0.79341743, -0.09352756,  1.14576686, ...,  1.074944  ,\n","         0.26975002,  1.30592   ],\n","       ...,\n","       [-1.18124428,  0.22839546, -1.00623184, ...,  0.67298999,\n","        -0.05374394,  0.58175999],\n","       [-0.2138425 ,  0.82379838, -0.96039976, ...,  0.78083999,\n","        -0.43185993,  0.919652  ],\n","       [ 1.03910661,  0.90790557, -0.0365134 , ...,  1.35398   ,\n","         0.27280008,  1.73144001]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["input_embeddings.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wCMkW6MgU2rG","executionInfo":{"status":"ok","timestamp":1737219323131,"user_tz":-330,"elapsed":387,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}},"outputId":"08db9d3e-cf20-4fea-b61b-c10d45328e85"},"id":"wCMkW6MgU2rG","execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 300)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":10,"id":"61e1a738-bdb7-43ce-81e1-f9d177a98dbb","metadata":{"id":"61e1a738-bdb7-43ce-81e1-f9d177a98dbb","executionInfo":{"status":"ok","timestamp":1737219331219,"user_tz":-330,"elapsed":372,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"outputs":[],"source":["# Step 4: Create Query, Key, and Value matrices\n","W_query = np.random.rand(embedding_dim, embedding_dim)\n","W_key = np.random.rand(embedding_dim, embedding_dim)\n","W_value = np.random.rand(embedding_dim, embedding_dim)"]},{"cell_type":"code","execution_count":11,"id":"b6c795cd-9bd9-4e0d-b6cf-e1a2dba5387b","metadata":{"id":"b6c795cd-9bd9-4e0d-b6cf-e1a2dba5387b","executionInfo":{"status":"ok","timestamp":1737219334055,"user_tz":-330,"elapsed":438,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"outputs":[],"source":["# Step 5: Compute Q, K, V\n","Q = np.dot(input_embeddings, W_query)\n","K = np.dot(input_embeddings, W_key)\n","V = np.dot(input_embeddings, W_value)"]},{"cell_type":"code","execution_count":12,"id":"f051d889-80e2-4a59-913a-f8984f5df5e0","metadata":{"id":"f051d889-80e2-4a59-913a-f8984f5df5e0","executionInfo":{"status":"ok","timestamp":1737219338622,"user_tz":-330,"elapsed":357,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"outputs":[],"source":["# Step 6: Compute attention scores\n","attention_scores = np.dot(Q, K.T)"]},{"cell_type":"code","execution_count":13,"id":"b3160a85-262c-4a22-ba7c-61aef4becfa5","metadata":{"id":"b3160a85-262c-4a22-ba7c-61aef4becfa5","executionInfo":{"status":"ok","timestamp":1737219340681,"user_tz":-330,"elapsed":369,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"outputs":[],"source":["# Step 7: Scale the attention scores\n","attention_scores /= np.sqrt(embedding_dim)"]},{"cell_type":"code","execution_count":14,"id":"e282852b-0dfe-480a-8cc0-837c243d251e","metadata":{"id":"e282852b-0dfe-480a-8cc0-837c243d251e","executionInfo":{"status":"ok","timestamp":1737219342563,"user_tz":-330,"elapsed":386,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"outputs":[],"source":["# Step 8: Apply softmax to get attention weights\n","attention_weights = softmax(attention_scores, axis=1)"]},{"cell_type":"code","execution_count":15,"id":"6a890a0d-b7eb-4b3e-a78a-c4d5683aea2d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6a890a0d-b7eb-4b3e-a78a-c4d5683aea2d","executionInfo":{"status":"ok","timestamp":1737219350301,"user_tz":-330,"elapsed":356,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}},"outputId":"f5e400a8-beca-4962-9097-072ac30f5dec"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Self-Attention Results:\n","\n","Word: The\n","Top 3 words this word pays attention to:\n","  chased: 1.0000\n","  The: 0.0000\n","  dog: 0.0000\n","\n","Word: dog\n","Top 3 words this word pays attention to:\n","  chased: 1.0000\n","  The: 0.0000\n","  dog: 0.0000\n","\n","Word: chased\n","Top 3 words this word pays attention to:\n","  chased: 1.0000\n","  The: 0.0000\n","  dog: 0.0000\n","\n","Word: the\n","Top 3 words this word pays attention to:\n","  chased: 1.0000\n","  The: 0.0000\n","  dog: 0.0000\n","\n","Word: cat\n","Top 3 words this word pays attention to:\n","  chased: 1.0000\n","  The: 0.0000\n","  dog: 0.0000\n","\n","Word: which\n","Top 3 words this word pays attention to:\n","  chased: 1.0000\n","  The: 0.0000\n","  dog: 0.0000\n","\n","Word: was\n","Top 3 words this word pays attention to:\n","  chased: 1.0000\n","  The: 0.0000\n","  dog: 0.0000\n","\n","Word: scared\n","Top 3 words this word pays attention to:\n","  chased: 1.0000\n","  The: 0.0000\n","  dog: 0.0000\n","\n","Interesting relationships:\n","'The' pays most attention to 'chased'\n","'dog' pays most attention to 'chased'\n","'the' pays most attention to 'chased'\n","'cat' pays most attention to 'chased'\n","'which' pays most attention to 'chased'\n","'was' pays most attention to 'chased'\n","'scared' pays most attention to 'chased'\n","\n","Attention Weight Matrix:\n","       The       dog    chased       the       cat     which       was    scared\n","       The      0.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00\n","       dog      0.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00\n","    chased      0.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00\n","       the      0.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00\n","       cat      0.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00\n","     which      0.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00\n","       was      0.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00\n","    scared      0.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00\n"]}],"source":["# Step 9: Compute the weighted sum\n","output = np.dot(attention_weights, V)\n","\n","# Print results\n","print(\"\\nSelf-Attention Results:\")\n","for i, word in enumerate(words):\n","    print(f\"\\nWord: {word}\")\n","    print(f\"Top 3 words this word pays attention to:\")\n","    top_attention = sorted(enumerate(attention_weights[i]), key=lambda x: x[1], reverse=True)[:3]\n","    for idx, weight in top_attention:\n","        print(f\"  {words[idx]}: {weight:.4f}\")\n","\n","# Analyze relationships\n","print(\"\\nInteresting relationships:\")\n","for i, word in enumerate(words):\n","    max_attention = np.argmax(attention_weights[i])\n","    if i != max_attention:\n","        print(f\"'{word}' pays most attention to '{words[max_attention]}'\")\n","\n","\n","# Visualize attention weights\n","print(\"\\nAttention Weight Matrix:\")\n","for i, word in enumerate(words):\n","    print(f\"{word:>10}\", end=\"\")\n","print()\n","for i, word in enumerate(words):\n","    print(f\"{word:>10}\", end=\"\")\n","    for j in range(seq_length):\n","        print(f\"{attention_weights[i, j]:>10.2f}\", end=\"\")\n","    print()"]},{"cell_type":"code","execution_count":null,"id":"96e142e0-ef1c-461e-b136-2fd17ce40f4b","metadata":{"id":"96e142e0-ef1c-461e-b136-2fd17ce40f4b"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}