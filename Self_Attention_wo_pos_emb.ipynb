{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import numpy as np\n","import gensim.downloader as api\n","from scipy.special import softmax"],"metadata":{"id":"bnocOx9hghnb","executionInfo":{"status":"ok","timestamp":1737210581322,"user_tz":-330,"elapsed":1945,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Load pre-trained GloVe embeddings\n","print(\"Loading GloVe embeddings...\")\n","glove_vectors = api.load(\"glove-wiki-gigaword-300\")  # 300-dimensional GloVe embeddings\n","print(\"GloVe embeddings loaded.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Y7K6UNBjHlz","outputId":"d1c775b8-b303-4a83-da96-2c90f3132bd4","executionInfo":{"status":"ok","timestamp":1737210711817,"user_tz":-330,"elapsed":128960,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading GloVe embeddings...\n","[==================================================] 100.0% 376.1/376.1MB downloaded\n","GloVe embeddings loaded.\n"]}]},{"cell_type":"code","source":["len(glove_vectors)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tglhWGqWo0Fq","outputId":"832aa104-79e4-4fbe-8730-ebb9742be5aa","executionInfo":{"status":"ok","timestamp":1737210711817,"user_tz":-330,"elapsed":3,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["400000"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["if 'the' in glove_vectors:\n","  #print(glove_vectors['the'])\n","  print(len(glove_vectors['cat']))\n","  print(len(glove_vectors['the']))"],"metadata":{"id":"PrwdXkx-cJr4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736579978103,"user_tz":-330,"elapsed":574,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}},"outputId":"05e0c464-8817-410f-a676-f726995b074e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["300\n","300\n"]}]},{"cell_type":"code","source":["if 'cat' in glove_vectors:\n","  print(glove_vectors['cat'])\n","  print(len(glove_vectors['cat']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fnRAXDTKm557","outputId":"c4e459c3-6ef0-4d0b-fe9c-04243805c062","executionInfo":{"status":"ok","timestamp":1737210735910,"user_tz":-330,"elapsed":691,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.29353    0.33247   -0.047372  -0.12247    0.071956  -0.23408\n"," -0.06238   -0.0037192 -0.39462   -0.69411    0.36731   -0.12141\n"," -0.044485  -0.15268    0.34864    0.22926    0.54361    0.25215\n","  0.097972  -0.087305   0.87058   -0.12211   -0.079825   0.28712\n"," -0.68563   -0.27265    0.22056   -0.75752    0.56293    0.091377\n"," -0.71004   -0.3142    -0.56826   -0.26684   -0.60102    0.26959\n"," -0.17992    0.10701   -0.57858    0.38161   -0.67127    0.10927\n","  0.079426   0.022372  -0.081147   0.011182   0.67089   -0.19094\n"," -0.33676   -0.48471   -0.35406   -0.15209    0.44503    0.46385\n","  0.38409    0.045081  -0.59079    0.21763    0.38576   -0.44567\n","  0.009332   0.442      0.097062   0.38005   -0.11881   -0.42718\n"," -0.31005   -0.025058   0.12689   -0.13468    0.11976    0.76253\n","  0.2524    -0.26934    0.068629  -0.10071    0.011066  -0.18532\n","  0.44983   -0.57507    0.12278   -0.064878   0.044456  -0.020999\n"," -0.069838  -0.47329   -0.43074    0.39158   -0.047815  -0.93659\n"," -0.55128   -0.1422    -0.15829    0.15623    0.070461   0.19892\n","  0.18942   -0.19339   -0.46594   -0.028825   0.0056752 -0.0054038\n","  0.43144    0.12257   -0.2611     0.04847    0.32244   -0.31064\n"," -0.10559    0.97954    0.069626  -0.023187  -0.86293    0.48273\n","  0.23649   -0.0034704 -0.18932    0.18588    0.023211  -0.30643\n"," -0.35717    0.19605   -0.1584    -0.0058626  0.35248    0.036053\n"," -0.53933    0.49435    0.45332   -0.18477    0.040648  -0.094517\n"," -0.07116    0.74005   -0.11465   -0.26916    0.089765  -0.25205\n"," -0.21469   -0.38847    0.32509    0.25773   -0.51764   -0.38457\n","  0.028254  -0.21232   -0.27311    0.69178   -0.37681    0.14241\n"," -0.24926    0.40314   -0.052916   0.07684    0.2135     0.10921\n","  0.049658   0.02093    0.11953    0.28648    0.87791    0.085838\n","  0.31983    0.51856   -0.22628    0.12402    0.48805    0.22111\n"," -0.52021    0.0025106 -0.13305   -0.052565   0.32744    0.64985\n","  0.072426  -0.52743   -0.20913   -0.27897   -0.10834   -0.10103\n","  0.15299   -0.36681    0.082445   0.1739    -0.28099   -0.069136\n","  0.7895     0.060571   0.38693   -0.16495   -0.21801    0.33288\n"," -0.44568   -0.49892   -0.34438   -0.035606  -0.24239   -0.4747\n"," -0.17254    0.071349   1.4091     0.46166    0.46546   -0.30979\n","  0.37203    0.47897   -0.28872   -0.65515   -0.13629   -0.14287\n"," -0.04843   -0.12786    0.18941   -0.037051   0.59471   -0.0051618\n"," -0.0086009 -0.33313    0.288     -0.058965  -0.67275    0.15544\n","  0.074187  -0.36441   -0.021285  -0.065337   0.13827    0.008395\n"," -0.041113   0.29401   -0.10344   -0.052371  -0.63084    0.16311\n","  0.052826  -0.021797  -0.28115   -0.078361  -0.38124    0.078089\n","  0.38411   -0.34629   -0.4322     0.091731  -0.67867   -0.041138\n"," -0.53981    0.10678    0.03343    0.81396   -0.19448    0.026248\n"," -0.14215    0.2954     0.62738    0.26499    0.6191    -0.04113\n","  0.12301    0.3158     0.10698    0.023654  -0.41355    0.034852\n","  0.21361    0.045834   0.053415  -0.36421    0.19707    0.50916\n"," -0.1949    -0.18788   -0.24449   -0.63397   -0.23125   -0.18823\n"," -1.0601     0.47794   -1.0102     0.24604   -0.4876     0.79146\n"," -0.11047   -0.21762   -0.6178     0.27815   -0.098169  -0.063205\n","  0.066069  -0.69305   -0.25928    0.44591   -0.64198   -0.33084\n"," -0.30154   -0.56359    0.60501   -0.09673    0.44444    0.22007  ]\n","300\n"]}]},{"cell_type":"code","source":["# Our input sentence\n","sentence = \"The dog chased the cat which was scared\"\n","words = sentence.split()\n","words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uxy_4LngjKXY","outputId":"81d42789-e720-4e60-ca44-b09dad17f315","executionInfo":{"status":"ok","timestamp":1737210797734,"user_tz":-330,"elapsed":480,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['The', 'dog', 'chased', 'the', 'cat', 'which', 'was', 'scared']"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Step 1: Create word embeddings using GloVe\n","embedding_dim = 300  # GloVe vectors are 300-dimensional\n","word_embeddings = np.array([glove_vectors[word.lower()] if word.lower() in glove_vectors else glove_vectors['unk'] for word in words])"],"metadata":{"id":"xfY-30XBjLLf","executionInfo":{"status":"ok","timestamp":1737211143149,"user_tz":-330,"elapsed":505,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Step 2: Create Query, Key, and Value matrices\n","\n","W_query = np.random.rand(embedding_dim, embedding_dim)\n","W_key = np.random.rand(embedding_dim, embedding_dim)\n","W_value = np.random.rand(embedding_dim, embedding_dim)\n"],"metadata":{"id":"UTfdS8zqjOkR","executionInfo":{"status":"ok","timestamp":1737211144204,"user_tz":-330,"elapsed":1,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["W_query"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDPwqg8LPOtO","executionInfo":{"status":"ok","timestamp":1737211145689,"user_tz":-330,"elapsed":3,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}},"outputId":"a10a9721-64f8-46fc-9f70-e69e7a37b9b2"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.10468889, 0.66985945, 0.78306423, ..., 0.59049432, 0.65616668,\n","        0.50552995],\n","       [0.6497369 , 0.16131284, 0.16950533, ..., 0.12476515, 0.76795722,\n","        0.62249402],\n","       [0.17199248, 0.5605414 , 0.86961772, ..., 0.33535591, 0.65218341,\n","        0.77357347],\n","       ...,\n","       [0.42978207, 0.52834508, 0.44224166, ..., 0.57845809, 0.61938   ,\n","        0.65871591],\n","       [0.17675379, 0.76667296, 0.96973226, ..., 0.96413132, 0.25804884,\n","        0.25553019],\n","       [0.22106515, 0.55787563, 0.28070566, ..., 0.18544292, 0.76325196,\n","        0.14542941]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["W_query.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BYXMH1YgZRSZ","executionInfo":{"status":"ok","timestamp":1737211147483,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}},"outputId":"6bdedb49-3562-42cc-d27f-6d8b56585c54"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(300, 300)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# Step 3: Compute Q, K, V\n","Q = np.dot(word_embeddings, W_query)\n","K = np.dot(word_embeddings, W_key)\n","V = np.dot(word_embeddings, W_value)"],"metadata":{"id":"5WfDD5rYjP3N","executionInfo":{"status":"ok","timestamp":1737211148759,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["Q.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxD4vlDlab5c","executionInfo":{"status":"ok","timestamp":1737211150092,"user_tz":-330,"elapsed":581,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}},"outputId":"fe3c4917-98d6-41aa-ba88-26ad9b1f7109"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 300)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Step 4: Compute attention scores\n","attention_scores = np.dot(Q, K.T)"],"metadata":{"id":"2WLC2ydUjQ07","executionInfo":{"status":"ok","timestamp":1737211401893,"user_tz":-330,"elapsed":664,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Step 5: Scale the attention scores\n","attention_scores /= np.sqrt(embedding_dim)"],"metadata":{"id":"WMguh5w4jTiR","executionInfo":{"status":"ok","timestamp":1737211403118,"user_tz":-330,"elapsed":3,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Step 6: Apply softmax to get attention weights\n","attention_weights = softmax(attention_scores, axis=1)"],"metadata":{"id":"nfQ6CrkhjVHG","executionInfo":{"status":"ok","timestamp":1737211403118,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Step 7: Compute the weighted sum\n","output = np.dot(attention_weights, V)"],"metadata":{"id":"h-z1-k0NjV6K","executionInfo":{"status":"ok","timestamp":1737211404129,"user_tz":-330,"elapsed":1,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["output.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kCMZ7sYoqHIP","outputId":"55feec14-7917-4ad7-dc44-1822b1e54857","executionInfo":{"status":"ok","timestamp":1737211405699,"user_tz":-330,"elapsed":1,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 300)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# Print results\n","for i, word in enumerate(words):\n","    print(f\"\\nWord: {word}\")\n","    print(f\"Top 5 words this word pays attention to:\")\n","    top_attention = sorted(enumerate(attention_weights[i]), key=lambda x: x[1], reverse=True)[:5]\n","    for idx, weight in top_attention:\n","        print(f\"  {words[idx]}: {weight:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ksFLELU1jcjA","outputId":"346a3c71-e2a8-4d73-9059-6f7820c5262a","executionInfo":{"status":"ok","timestamp":1737211406738,"user_tz":-330,"elapsed":3,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Word: The\n","Top 5 words this word pays attention to:\n","  The: 0.5000\n","  the: 0.5000\n","  chased: 0.0000\n","  which: 0.0000\n","  scared: 0.0000\n","\n","Word: dog\n","Top 5 words this word pays attention to:\n","  cat: 1.0000\n","  dog: 0.0000\n","  was: 0.0000\n","  which: 0.0000\n","  scared: 0.0000\n","\n","Word: chased\n","Top 5 words this word pays attention to:\n","  The: 0.5000\n","  the: 0.5000\n","  chased: 0.0000\n","  which: 0.0000\n","  scared: 0.0000\n","\n","Word: the\n","Top 5 words this word pays attention to:\n","  The: 0.5000\n","  the: 0.5000\n","  chased: 0.0000\n","  which: 0.0000\n","  scared: 0.0000\n","\n","Word: cat\n","Top 5 words this word pays attention to:\n","  cat: 1.0000\n","  dog: 0.0000\n","  was: 0.0000\n","  scared: 0.0000\n","  which: 0.0000\n","\n","Word: which\n","Top 5 words this word pays attention to:\n","  cat: 0.9810\n","  was: 0.0136\n","  dog: 0.0033\n","  which: 0.0020\n","  scared: 0.0001\n","\n","Word: was\n","Top 5 words this word pays attention to:\n","  cat: 1.0000\n","  dog: 0.0000\n","  was: 0.0000\n","  which: 0.0000\n","  scared: 0.0000\n","\n","Word: scared\n","Top 5 words this word pays attention to:\n","  was: 0.4775\n","  cat: 0.2819\n","  dog: 0.1935\n","  which: 0.0343\n","  The: 0.0046\n"]}]},{"cell_type":"code","source":["# Analyze relationships\n","print(\"\\nInteresting relationships:\")\n","for i, word in enumerate(words):\n","    max_attention = np.argmax(attention_weights[i])\n","    if i != max_attention:\n","        print(f\"'{word}' pays most attention to '{words[max_attention]}'\")"],"metadata":{"id":"LPLNa8AfNGJQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4f8c1144-15c3-4d5c-82a4-fdfd27c3e7d3","executionInfo":{"status":"ok","timestamp":1737211415129,"user_tz":-330,"elapsed":460,"user":{"displayName":"Shubham Pandey","userId":"10645658922718505789"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Interesting relationships:\n","'dog' pays most attention to 'cat'\n","'chased' pays most attention to 'The'\n","'the' pays most attention to 'The'\n","'which' pays most attention to 'cat'\n","'was' pays most attention to 'cat'\n","'scared' pays most attention to 'was'\n"]}]}]}